<!doctype html>
<html>
<head>
    <!--Setting-->
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, user-scalable=no, initial-scale=1.0, maximum-scale=1.0, minimum-scale=1.0">
    <meta http-equiv="X-UA-Compatible" content="ie=edge">
    <meta name="renderer" content="webkit|ie-comp|ie-stand">
    <meta name="apple-mobile-web-app-capable"  content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black">
    <meta name="format-detection" content="telephone=no,email=no">
    
    
    <!--Simple SEO-->

<meta name="description" content=学习使我快乐/>


<meta name="robots" content=all />
<meta name="google" content=all />
<meta name="googlebot" content=all />
<meta name="verify" content=all />
    <!--Title-->

<title>逻辑图解Spark wordCount过程 | Fred&#39;s Blog</title>

<link rel="alternate" href="/atom.xml" title="Fred&#39;s Blog" type="application/atom+xml">


<link rel="icon" href="/favicon.ico">

    
<link rel="stylesheet" href="/css/base.css">
<link rel="stylesheet" href="/css/pages/post.css">
<link rel="stylesheet" href="//cdn.bootcss.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/css/thirdParty/highlight/github.css">
<link rel="stylesheet" href="/css/pages/comments.css">

    <!--script-->



<script async src="//dn-lbstatics.qbox.me/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<!--<script src="https://imsun.github.io/gitment/dist/gitment.browser.js"></script>-->


<script src="/js/thirdParty/gitment.browser.min.js"></script>
<script>
  var labels = "blog,gitment";
  labels = labels.split(",");
  var gitment = new Gitment({
    id: window.location.pathname,
    owner: "Fred",
    repo: "6high.github.io",
    oauth: {
      client_id: "4f5b46aa373e98aef081",
      client_secret: "b3c681e47eff7a54549b09b0d42a439fa9addf57"
    },
    perPage: "20",
    labels: labels
  });
</script>

    
    
</head>

<body id="normal">
<!--[if lte IE 8]>
<style>
    html{ font-size: 1em }
</style>
<![endif]-->
<!--[if lte IE 9]>
<style>
    header{ top: 71px; position: absolute!important;}
    #container{padding-top: 151px!important;}
</style>
<div style="position:fixed;z-index:9999;left:0;top:0;width:100%;height:70px;background-color:#e0e0e0;color:#396CA5;border-bottom:1px solid #cecece;text-align:center;line-height:70px;white-space: nowrap;overflow: hidden;text-overflow: ellipsis">你使用的浏览器版本过低，为了你更好的阅读体验，请更新浏览器的版本或者使用其他现代浏览器，比如Chrome、Firefox、Safari等。</div>
<![endif]-->

<div id="wrap">
    <header  style="position: absolute;" >
    <div id="site-meta">
        <a href="/" id="logo">
            <h1 class="title">Fred&#39;s Blog</h1>
        </a>
        
        <h2 class="subtitle">记录学习Apache-Spark的过程</h2>
        
    </div>
    <ul id="nav">
        
            <li><a href="/"><i class="fa fa-home"></i>首页</a></li>
        
            <li><a href="/atom.xml"><i class="fa fa-rss"></i>RSS</a></li>
        
        <li id="search"><a href="javascript:void(0)"><i class="fa fa-search"></i>搜索</a></li>
    </ul>
</header>

    <div id="container">
        
<ul id="sidebar">
    
    
    
    
<li class="widget widget-normal category">
    <h3 class="fa fa-th widget-title">分类</h3>
    <ul class="category-list"><li class="category-list-item"><a class="category-list-link current" href="/categories/spark/"><i class="fa" aria-hidden="true">spark</i></a><span class="category-list-count">1</span><ul class="category-list-child"><li class="category-list-item"><a class="category-list-link current" href="/categories/spark/RDD/"><i class="fa" aria-hidden="true">RDD</i></a><span class="category-list-count">1</span></li></ul></li></ul>
</li>


    
    
<li class="widget widget-normal archive">
  <h3 class="fa fa-archive widget-title">归档</h3>
    <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2018/03/"><i class="fa" aria-hidden="true">三月 2018</i></a><span class="archive-list-count">1</span></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2017/05/"><i class="fa" aria-hidden="true">五月 2017</i></a><span class="archive-list-count">1</span></li></ul>
</li>


    
    
<li class="widget widget-normal popular-posts" id="popular-posts">
    <h3 class="fa fa-thermometer-3 widget-title">热门文章</h3>
    <ul id="popular-content">
        <li class="load-first"><i class="fa fa-spinner fa-pulse"></i></li>
    </ul>
</li>

    
    
<li class="widget widget-normal tags">
  <h3 class="fa fa-tags widget-title">标签云</h3>
  <div class="tagcloud-content">
    
      <a href="/tags/RDD/" style="font-size: 0.14rem; color: #69c">RDD</a> <a href="/tags/spark/" style="font-size: 0.14rem; color: #69c">spark</a>
  </div>
</li>


    
    
<li class="widget widget-normal friends-link">
    <h3 class="fa fa-globe widget-title">友链</h3><br/>

    
        <a href="http://geekaholiclin.github.io" class="fa" target="_blank">主题作者</a>

    
        <a href="http://liuwx.xyz/" class="fa" target="_blank">liuwx</a>

    

</li>

    
</ul>


        <div id="main">
    <article id="post">
        <div id="post-header">

            <h1 id="逻辑图解Spark wordCount过程">
                
                逻辑图解Spark wordCount过程
                
            </h1>
            <div class="article-meta">
    
    
    <span class="categories-meta fa-wrap">
            <i class="fa fa-folder-open-o"></i>
        <span>RDD</span>
    </span>
    
    
    <span class="fa-wrap">
         <i class="fa fa-tags"></i>
        <span class="tags-meta">
            
            RDD
            
        </span>
    </span>
    
    
    <span class="fa-wrap">
        <i class="fa fa-clock-o"></i>
        <span class="date-meta ">2018/03/10</span>
    </span>
    
    
    <span class="fa-wrap">
            <i class="fa fa-thermometer-three-quarters"></i>
        <span class="hits hits-meta " data-leadcloud-title="逻辑图解Spark wordCount过程"
              data-leadcloud-url="/2018/03/10/图解Spark wordCount过程/"><i class="fa fa-spinner fa-spin"></i></span>
    </span>
    
    
</div>

            
            
        </div>
        
        <div id="post-body">
            <p>学习每一门新的语言都会先学习一个<code>helloworld</code>程序,而<a href="http://spark.apache.org/" target="_blank" rel="noopener">Spark官网</a>将world count作为一个入门程序,本屌也是一个spark菜鸟,那么就从一个word count开始吧!</p>
<p>==本文旨在用图解数据流程的方式帮助Spark新手理解RDD分区和基本算子==</p>
<h3 id="1-程序及数据文件"><a href="#1-程序及数据文件" class="headerlink" title="1.程序及数据文件"></a>1.程序及数据文件</h3><p>首先我们先填写一个包含单词的文件<code>helloSpark</code>,内容如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello Apache</span><br><span class="line">hello Spark</span><br><span class="line">hello Hadoop</span><br><span class="line">I love Spark</span><br></pre></td></tr></table></figure>
<p>这个文件有三行,将作为我们word count程序的输入文件.</p>
<p>Word count程序答代码如下,代码中包含注释,方便大家理解.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.hust.gao.wordCount</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  	 <span class="comment">// 1.启动spark</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="comment">// 2.读取本地文件</span></span><br><span class="line">    <span class="keyword">val</span> input = <span class="string">"hdfs:///data/helloSpark"</span></span><br><span class="line">    <span class="comment">// 3.读取数据,将数据分为三个区域,并缓存</span></span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(input, <span class="number">3</span>)</span><br><span class="line">    lines.cache()</span><br><span class="line">    <span class="comment">// 4.对每个分区的每一行数据进行切分,并压平</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="comment">// 5.将单词映射成K-V形式</span></span><br><span class="line">    <span class="keyword">val</span> kv1 = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 6.对相同key的value进行合并</span></span><br><span class="line">    <span class="keyword">val</span> wordCount = kv1.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 7.对单词数量进行排序</span></span><br><span class="line">    <span class="keyword">val</span> result = wordCount.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">// 8.收集数据打印</span></span><br><span class="line">    result.collect().foreach(println)</span><br><span class="line">    <span class="comment">// 让程序沉睡10000000毫秒,我们可以去 http://localhost:4040</span></span><br><span class="line">    <span class="comment">// 查看Spark UI界面</span></span><br><span class="line">    <span class="comment">// 关闭spark程序</span></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">10000000</span>)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<h3 id="2-源码分析和逻辑图解"><a href="#2-源码分析和逻辑图解" class="headerlink" title="2.源码分析和逻辑图解"></a>2.源码分析和逻辑图解</h3><ul>
<li><ol>
<li>textFile</li>
</ol>
</li>
</ul>
<p>当我们通过<code>SparkContext</code>启动spark后,可通过<code>SparkContext.textFile()</code>读取数据源(可以是本地 file,内存数据结构, HDFS,HBase 等)的数据创建最初的<code>RDD</code>.本文从HDFS中读取数据,路径为<code>/data/helloSpark</code>,接下来我们阅读下<code>SparkContext.textFile()</code>的源码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>textFile()</code>需要数据路径<code>path</code>和分区数量<code>minPartitions</code>这两个参数,内部则是调用了<code>hadoopFile()</code>从HDFS中读取数据生成的<code>HadoopRDD[(LongWritable, Text)]</code>,经过<code>map</code>操作对每个分区的每行数据处理后获得<code>MapPartitionsRDD[String]</code><br>那么我们根据RDD lineage(RDD的血缘关系链)可以把<code>sc.textFile(input, 3)</code>的逻辑图画出来,如下:(<strong>矩形框代表RDD,矩形内部的圆和椭圆代表RDD的每个分区,圆形内是一行行的数据,这些分区分布不同的物理机器上</strong>)</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-1.textFile.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="1.textFile"></p>
<p><strong>注:</strong> 数据从HDFS中读取到内存中,会存在网络传输和交换数据,所以图中 HDFS -&gt; HadoopRDD 的箭头颜色为咖啡色;有网络交换数据的RDD会被标记成蓝色.</p>
<ul>
<li><ol>
<li>flatMap 和 map</li>
</ol>
</li>
</ul>
<p>通过<code>sc.textFile(input, 3)</code>把数据每一行读取成分为三个分区的<code>lines: RDD[String]</code>后,我们要将每一行数据通过<code>空格</code>进行切分,压平成每个单词,并对每个单词标记为<code>(word, 1)</code>这种K-V形式.下面我们阅读<code>flatMap</code>和<code>map</code>的源码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从RDD的最简单的两个算子我们可以看出,RDD(Resilient Distributed Datasets),弹性分布式数据集其实就是跑在不同节点分区(不同物理机器)中的迭代器,通过网络通信将<code>Iterator</code>抽象成集合.<br><code>flatMap</code>和<code>map</code>的底层是RDD的每个分区(<code>MapPartitionsRDD</code>)中,调用scala迭代器的<code>Iterator.flatMap()</code>和<code>Iterator.map()</code>方法对该分区的数据进行处理.那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-2.flatMap.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="2.flatMap"></p>
<ul>
<li><ol>
<li>reduceByKey</li>
</ol>
</li>
</ul>
<p>通过<code>lines.flatMap(line =&gt; line.split(&quot; &quot;))</code>对每行切分压平成单个单词后,使用<code>words.map(word =&gt; (word, 1))</code>将单词转换成K-V形式,接下来就是对相同的单词进行聚合,使用RDD的<code>reduceByKey</code>算子.我们先来看看源码,这次要带上注释了,因为这是wordCount最核心最核心的算子.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">   * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">   * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/</span></span><br><span class="line"><span class="comment">   * parallelism level.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>通过阅读源码和注释,我们可以知道:</p>
<p>1.<code>reduceByKey</code>需要传入一个自定义聚合函数<code>func: (V, V) =&gt; V)</code>,这个聚合函数将会把两个相同 key 的 value 进行聚合. 如果我的自定义聚合函数是<code>func: (_ + _)</code>,即返回两个参数之和,那么<code>(hello, 1)</code>和<code>(hello, 1)</code>这两个key相同的数据,将会被聚合成<code>(hello, 2)</code></p>
<p>2.<code>reduceByKey</code>这个算子第一步将会在<strong>map端</strong>(即每个分区中)使用自定义聚合函数<code>func: (V, V) =&gt; V)</code>先聚合一次;</p>
<p>第二步将会对每个分区的key进行哈希运算,相同的key一定会被映射到相同的分区,不同的key可能被映射到相同的分区,然后将数据传输(shuffle)给指定的分区(reduce端)上去;</p>
<p>第三步,在reduce端,再调用制定与聚合函数<code>func: (V, V) =&gt; V)</code>即可完成RDD的聚合.</p>
<p>经过继续查看源码,我画出了如下逻辑图,大家先理解下.如下:</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-3.reduceByKey.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="3.reduceByKey"></p>
<p>我们可以看到<code>reduceByKey</code>还调用了<code>reduceByKey(defaultPartitioner(self), func)</code>,后面又调用了<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>,那么我们再来看看源码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">  combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">  <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">    <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    self.context.clean(createCombiner),</span><br><span class="line">    self.context.clean(mergeValue),</span><br><span class="line">    self.context.clean(mergeCombiners))</span><br><span class="line">  <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">    self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">    &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)</span><br><span class="line">      .setSerializer(serializer)</span><br><span class="line">      .setAggregator(aggregator)</span><br><span class="line">      .setMapSideCombine(mapSideCombine)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到<code>reduceByKey</code>最终调用的是<code>combineByKeyWithClassTag()</code>,<code>combineByKe</code>是很多RDD聚合算子的底层函数,学会<code>combineByKey</code>算子就学懂了spark基础的一半.</p>
<p>首先,<code>combineByKeyWithClassTag()</code>是强类型的算子,往里面传的自定义函数必须声明类型;其次这个算子有两个聚合过程,分别是<code>map端</code>和<code>reduce端</code>聚合,可以接受两个不同的聚合函数(<code>map端</code>和<code>reduce端</code>进行不同的聚合);第三必须传入一个将初始数据转换类型为聚合之后的类型的函数;第四,可以制定与分区器.</p>
<p>关于RDD中非常重要,Spark中非常重要的<code>combineByKeyWithClassTag()</code>算子,我之后也会出博客进行细讲!</p>
<ul>
<li><ol>
<li>sortBy</li>
</ol>
</li>
</ul>
<p>当我们算出了每个单词的数量后,剩下的任务就是排序了.Spark 的RDD 和javaRDD 提供了<code>sortByKey()</code>算子,但是我们的属性形式是<code>(word,counts)</code>,我们的单词才是key.如果是使用java,我们就只能在调用<code>sortByKey()</code>前将数据的K-V转调换为V-K.但是在scala中,提供了更加人性化的<code>sortBy()</code>算子,源码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">    f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">    ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">    (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.keyBy[<span class="type">K</span>](f)</span><br><span class="line">      .sortByKey(ascending, numPartitions)</span><br><span class="line">      .values</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">    : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">val</span> part = <span class="keyword">new</span> <span class="type">RangePartitioner</span>(numPartitions, self, ascending)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, part)</span><br><span class="line">    .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>sortBy()</code>通过隐式转换(<code>implicit</code>)来控制排序,内部还是调用的<code>sortByKey()</code>, <code>sortByKey</code>会调用<code>RangePartitioner</code>分区器,将比较值的映射到不同的节点,数据计算交换传输(shuffle)到对应的节点,再在每个分区上进行排序.ascending=true表示升序，false表示降序。</p>
<p>那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-4.sortBy.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="4.sortBy"></p>
<h3 id="3-全程逻辑图"><a href="#3-全程逻辑图" class="headerlink" title="3.全程逻辑图"></a>3.全程逻辑图</h3><p>高清的wordCount全程逻辑图,方便大家查看.</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-5.wordCount_logical_plan.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="5.wordCount_logical_plan"></p>
<p>最后感谢<a href="https://github.com/JerryLead" target="_blank" rel="noopener">@JerryLead</a>的Spark文档!</p>

        </div>
        <div id="post-footer">
            <div class="avatar" >
                <img src="http://oj7dzxd3r.bkt.clouddn.com/qrcode_learningSpark_430.jpg" alt="avatar"/>
                
                
                <a href="javascript:void(0)" class="donate fa">赠我一杯 &#128536;</a>
                
            </div>
            <ul class="author-profile-section">
                <li>
                  
                  作者:
                  
                    
                    <a href="/about.html">Fred</a>
                </li>
                
                <li>发表日期: <span>2018-03-10  00:00:00</span></li>
                
                <li>最后编辑日期: <span>2018-03-13  19:10:03</span></li>
                
                <li class="post-category">
                    文章分类:
                    
                    <a href="/categories/spark/">spark</a>
                    
                    <a href="/categories/spark/RDD/">RDD</a>
                    
                </li>
                <li class="post-tags">
                    文章标签:
                    
                    <a href="/tags/RDD/">RDD</a>
                    
                    <a href="/tags/spark/">spark</a>
                    
                </li>
                
                <li> 版权声明: <a href="https://creativecommons.org/licenses/by-nc-nd/3.0/" target="_blank">
知识共享署名-非商业性使用-禁止演绎 3.0 未本地化版本许可协议（CC BY-NC-ND 3.0）
</a></li>
                
            </ul>
            <div id="donate-wrap">
                
                
                
                <img src="http://oj7dzxd3r.bkt.clouddn.com/aliPay.png" alt="支付宝付款" class="donate-img">
                
                <img src="http://oj7dzxd3r.bkt.clouddn.com/WechatPay.jpeg" alt="微信付款" class="donate-img">
                
                
            </div>
        </div>
    </article>
    <div class="article-nav">
        
        
        <a href="/2017/05/16/hello-world/" class="next-post fa">Hello World</a>
        
    </div>
    
    <div id="comments">
        

<script>
  gitment.render(document.getElementById("comments"));
</script>



    </div>
    
</div>


    </div>
    <footer id="footer">
    
    <div class="social">
        
        <a href="mailto:gao.liu.cn@gmail.com" class="fa fa-envelope-o" target="_blank" title="mail to me"></a>
        
        <a href="https://github.com/6high" class="fa fa-github" target="_blank" title="Follow me~"></a>
        
    </div>
    
    <div>
        
        <a href="/" class="copyright-links">Fred</a>&copy;2017 - 2018.All Rights
        Reserved.
    </div>
    <p>Powered by <a href="https://hexo.io" class="copyright-links" target="_blank">Hexo</a> | Theme by <a
                href="https://github.com/GeekaholicLin" class="copyright-links" target="_blank">GeekaholicLin</a>
    </p>
    
    
    <p>
        <span id="busuanzi_container_site_uv" class="fa fa-bar-chart">
        欢迎第<span id="busuanzi_value_site_uv"><i class="fa fa-spinner fa-spin"></i></span>位小伙伴~
        </span>
    </p>
    
</footer>

</div>
    <ul id="tools">
    <li class="totop-btn fa fa-angle-up"></li>
    <li class="exchange-btn fa fa-exchange"></li>
  
    <li class="toc-btn fa fa-list-ul"></li>
    
    

    
    <li class="comment-btn fa fa-comments-o">
        <a href="#comments" title="comments"></a>
    </li>
    
</ul>
<p id="process"></p>
<div id="search-overlay">
    <div class="search-area-wrap">
        <div id="search-area">
            <div class="input-wrap focus">
                <i class="fa fa-search" aria-hidden="true"></i>
                <input id="search-input" autofocus autocomplete="off" type="text"
                       placeholder="search this website..."/>
            </div>
            <ul id="search-result">
                <li class="load-first"><i class="fa fa-spinner fa-pulse"></i></li>
            </ul>
        </div>
    </div>
</div>

    <ol class="toc"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-程序及数据文件"><span class="toc-number">1.</span> <span class="toc-text">1.程序及数据文件</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-源码分析和逻辑图解"><span class="toc-number">2.</span> <span class="toc-text">2.源码分析和逻辑图解</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-全程逻辑图"><span class="toc-number">3.</span> <span class="toc-text">3.全程逻辑图</span></a></li></ol>




<script src="/js/search.js"></script>
<script type="text/javascript">
    //theme config datas
    var copyrightObj = {};
    copyrightObj.enable = 'true';
    copyrightObj.triggerCopyLength = '200';
    copyrightObj.appendText = '版权声明：本文为博主原创文章，未经博主允许不得转载。 © example';
    var leancloudObj = {};
    leancloudObj.enable = 'true';
    leancloudObj.className = 'BlogCounter';
    leancloudObj.limits = '10';
</script>
<script type="text/javascript">
    var search = {};
    var search_path = "search.xml";
    if (!search_path) {
        search_path = "search.xml";
    }
    search.path = "/" + search_path;
    search.func =  _ajax.init();
</script>
<script src="/js/app.js"></script>


</body>
</html>