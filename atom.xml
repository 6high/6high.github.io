<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Fred&#39;s Blog</title>
  
  <subtitle>记录学习Apache-Spark的过程</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://6high.top/"/>
  <updated>2018-03-13T11:10:03.068Z</updated>
  <id>http://6high.top/</id>
  
  <author>
    <name>Fred</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>逻辑图解Spark wordCount过程</title>
    <link href="http://6high.top/2018/03/10/%E5%9B%BE%E8%A7%A3Spark%20wordCount%E8%BF%87%E7%A8%8B/"/>
    <id>http://6high.top/2018/03/10/图解Spark wordCount过程/</id>
    <published>2018-03-09T16:00:00.000Z</published>
    <updated>2018-03-13T11:10:03.068Z</updated>
    
    <content type="html"><![CDATA[<p>学习每一门新的语言都会先学习一个<code>helloworld</code>程序,而<a href="http://spark.apache.org/" target="_blank" rel="noopener">Spark官网</a>将world count作为一个入门程序,本屌也是一个spark菜鸟,那么就从一个word count开始吧!</p><p>==本文旨在用图解数据流程的方式帮助Spark新手理解RDD分区和基本算子==</p><h3 id="1-程序及数据文件"><a href="#1-程序及数据文件" class="headerlink" title="1.程序及数据文件"></a>1.程序及数据文件</h3><p>首先我们先填写一个包含单词的文件<code>helloSpark</code>,内容如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello Apache</span><br><span class="line">hello Spark</span><br><span class="line">hello Hadoop</span><br><span class="line">I love Spark</span><br></pre></td></tr></table></figure><p>这个文件有三行,将作为我们word count程序的输入文件.</p><p>Word count程序答代码如下,代码中包含注释,方便大家理解.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.hust.gao.wordCount</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">   <span class="comment">// 1.启动spark</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="comment">// 2.读取本地文件</span></span><br><span class="line">    <span class="keyword">val</span> input = <span class="string">"hdfs:///data/helloSpark"</span></span><br><span class="line">    <span class="comment">// 3.读取数据,将数据分为三个区域,并缓存</span></span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(input, <span class="number">3</span>)</span><br><span class="line">    lines.cache()</span><br><span class="line">    <span class="comment">// 4.对每个分区的每一行数据进行切分,并压平</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="comment">// 5.将单词映射成K-V形式</span></span><br><span class="line">    <span class="keyword">val</span> kv1 = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 6.对相同key的value进行合并</span></span><br><span class="line">    <span class="keyword">val</span> wordCount = kv1.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 7.对单词数量进行排序</span></span><br><span class="line">    <span class="keyword">val</span> result = wordCount.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">// 8.收集数据打印</span></span><br><span class="line">    result.collect().foreach(println)</span><br><span class="line">    <span class="comment">// 让程序沉睡10000000毫秒,我们可以去 http://localhost:4040</span></span><br><span class="line">    <span class="comment">// 查看Spark UI界面</span></span><br><span class="line">    <span class="comment">// 关闭spark程序</span></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">10000000</span>)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="2-源码分析和逻辑图解"><a href="#2-源码分析和逻辑图解" class="headerlink" title="2.源码分析和逻辑图解"></a>2.源码分析和逻辑图解</h3><ul><li><ol><li>textFile</li></ol></li></ul><p>当我们通过<code>SparkContext</code>启动spark后,可通过<code>SparkContext.textFile()</code>读取数据源(可以是本地 file,内存数据结构, HDFS,HBase 等)的数据创建最初的<code>RDD</code>.本文从HDFS中读取数据,路径为<code>/data/helloSpark</code>,接下来我们阅读下<code>SparkContext.textFile()</code>的源码.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>textFile()</code>需要数据路径<code>path</code>和分区数量<code>minPartitions</code>这两个参数,内部则是调用了<code>hadoopFile()</code>从HDFS中读取数据生成的<code>HadoopRDD[(LongWritable, Text)]</code>,经过<code>map</code>操作对每个分区的每行数据处理后获得<code>MapPartitionsRDD[String]</code><br>那么我们根据RDD lineage(RDD的血缘关系链)可以把<code>sc.textFile(input, 3)</code>的逻辑图画出来,如下:(<strong>矩形框代表RDD,矩形内部的圆和椭圆代表RDD的每个分区,圆形内是一行行的数据,这些分区分布不同的物理机器上</strong>)</p><p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-1.textFile.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="1.textFile"></p><p><strong>注:</strong> 数据从HDFS中读取到内存中,会存在网络传输和交换数据,所以图中 HDFS -&gt; HadoopRDD 的箭头颜色为咖啡色;有网络交换数据的RDD会被标记成蓝色.</p><ul><li><ol><li>flatMap 和 map</li></ol></li></ul><p>通过<code>sc.textFile(input, 3)</code>把数据每一行读取成分为三个分区的<code>lines: RDD[String]</code>后,我们要将每一行数据通过<code>空格</code>进行切分,压平成每个单词,并对每个单词标记为<code>(word, 1)</code>这种K-V形式.下面我们阅读<code>flatMap</code>和<code>map</code>的源码.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">&#125;</span><br><span class="line"> </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>从RDD的最简单的两个算子我们可以看出,RDD(Resilient Distributed Datasets),弹性分布式数据集其实就是跑在不同节点分区(不同物理机器)中的迭代器,通过网络通信将<code>Iterator</code>抽象成集合.<br><code>flatMap</code>和<code>map</code>的底层是RDD的每个分区(<code>MapPartitionsRDD</code>)中,调用scala迭代器的<code>Iterator.flatMap()</code>和<code>Iterator.map()</code>方法对该分区的数据进行处理.那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:</p><p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-2.flatMap.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="2.flatMap"></p><ul><li><ol><li>reduceByKey</li></ol></li></ul><p>通过<code>lines.flatMap(line =&gt; line.split(&quot; &quot;))</code>对每行切分压平成单个单词后,使用<code>words.map(word =&gt; (word, 1))</code>将单词转换成K-V形式,接下来就是对相同的单词进行聚合,使用RDD的<code>reduceByKey</code>算子.我们先来看看源码,这次要带上注释了,因为这是wordCount最核心最核心的算子.</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">   * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">   * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/</span></span><br><span class="line"><span class="comment">   * parallelism level.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>通过阅读源码和注释,我们可以知道:</p><p>1.<code>reduceByKey</code>需要传入一个自定义聚合函数<code>func: (V, V) =&gt; V)</code>,这个聚合函数将会把两个相同 key 的 value 进行聚合. 如果我的自定义聚合函数是<code>func: (_ + _)</code>,即返回两个参数之和,那么<code>(hello, 1)</code>和<code>(hello, 1)</code>这两个key相同的数据,将会被聚合成<code>(hello, 2)</code></p><p>2.<code>reduceByKey</code>这个算子第一步将会在<strong>map端</strong>(即每个分区中)使用自定义聚合函数<code>func: (V, V) =&gt; V)</code>先聚合一次;</p><p>第二步将会对每个分区的key进行哈希运算,相同的key一定会被映射到相同的分区,不同的key可能被映射到相同的分区,然后将数据传输(shuffle)给指定的分区(reduce端)上去;</p><p>第三步,在reduce端,再调用制定与聚合函数<code>func: (V, V) =&gt; V)</code>即可完成RDD的聚合.</p><p>经过继续查看源码,我画出了如下逻辑图,大家先理解下.如下:</p><p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-3.reduceByKey.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="3.reduceByKey"></p><p>我们可以看到<code>reduceByKey</code>还调用了<code>reduceByKey(defaultPartitioner(self), func)</code>,后面又调用了<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>,那么我们再来看看源码:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">  combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">  <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">    <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    self.context.clean(createCombiner),</span><br><span class="line">    self.context.clean(mergeValue),</span><br><span class="line">    self.context.clean(mergeCombiners))</span><br><span class="line">  <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">    self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">    &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)</span><br><span class="line">      .setSerializer(serializer)</span><br><span class="line">      .setAggregator(aggregator)</span><br><span class="line">      .setMapSideCombine(mapSideCombine)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>我们可以看到<code>reduceByKey</code>最终调用的是<code>combineByKeyWithClassTag()</code>,<code>combineByKe</code>是很多RDD聚合算子的底层函数,学会<code>combineByKey</code>算子就学懂了spark基础的一半.</p><p>首先,<code>combineByKeyWithClassTag()</code>是强类型的算子,往里面传的自定义函数必须声明类型;其次这个算子有两个聚合过程,分别是<code>map端</code>和<code>reduce端</code>聚合,可以接受两个不同的聚合函数(<code>map端</code>和<code>reduce端</code>进行不同的聚合);第三必须传入一个将初始数据转换类型为聚合之后的类型的函数;第四,可以制定与分区器.</p><p>关于RDD中非常重要,Spark中非常重要的<code>combineByKeyWithClassTag()</code>算子,我之后也会出博客进行细讲!</p><ul><li><ol><li>sortBy</li></ol></li></ul><p>当我们算出了每个单词的数量后,剩下的任务就是排序了.Spark 的RDD 和javaRDD 提供了<code>sortByKey()</code>算子,但是我们的属性形式是<code>(word,counts)</code>,我们的单词才是key.如果是使用java,我们就只能在调用<code>sortByKey()</code>前将数据的K-V转调换为V-K.但是在scala中,提供了更加人性化的<code>sortBy()</code>算子,源码如下:</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">    f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">    ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">    (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.keyBy[<span class="type">K</span>](f)</span><br><span class="line">      .sortByKey(ascending, numPartitions)</span><br><span class="line">      .values</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">    : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">val</span> part = <span class="keyword">new</span> <span class="type">RangePartitioner</span>(numPartitions, self, ascending)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, part)</span><br><span class="line">    .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><code>sortBy()</code>通过隐式转换(<code>implicit</code>)来控制排序,内部还是调用的<code>sortByKey()</code>, <code>sortByKey</code>会调用<code>RangePartitioner</code>分区器,将比较值的映射到不同的节点,数据计算交换传输(shuffle)到对应的节点,再在每个分区上进行排序.ascending=true表示升序，false表示降序。</p><p>那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:</p><p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-4.sortBy.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="4.sortBy"></p><h3 id="3-全程逻辑图"><a href="#3-全程逻辑图" class="headerlink" title="3.全程逻辑图"></a>3.全程逻辑图</h3><p>高清的wordCount全程逻辑图,方便大家查看.</p><p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-5.wordCount_logical_plan.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="5.wordCount_logical_plan"></p><p>最后感谢<a href="https://github.com/JerryLead" target="_blank" rel="noopener">@JerryLead</a>的Spark文档!</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;学习每一门新的语言都会先学习一个&lt;code&gt;helloworld&lt;/code&gt;程序,而&lt;a href=&quot;http://spark.apache.org/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Spark官网&lt;/a&gt;将world count作为一个入
      
    
    </summary>
    
      <category term="spark" scheme="http://6high.top/categories/spark/"/>
    
      <category term="RDD" scheme="http://6high.top/categories/spark/RDD/"/>
    
    
      <category term="RDD" scheme="http://6high.top/tags/RDD/"/>
    
      <category term="spark" scheme="http://6high.top/tags/spark/"/>
    
  </entry>
  
  <entry>
    <title>Hello World</title>
    <link href="http://6high.top/2017/05/16/hello-world/"/>
    <id>http://6high.top/2017/05/16/hello-world/</id>
    <published>2017-05-16T07:19:08.000Z</published>
    <updated>2018-03-13T11:12:54.369Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p><h3 id="new-Document"><a href="#new-Document" class="headerlink" title="new Document"></a>new Document</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 这是标题</span><br><span class="line">layout: post</span><br><span class="line">date: 2017-05-16 15:19:08</span><br><span class="line">comments: true</span><br><span class="line">tags: [test, spark]</span><br><span class="line">categories: [Paranoid, Web] </span><br><span class="line">keywords: [hexo, blog]</span><br><span class="line">---</span><br><span class="line"># this my first test</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Welcome to &lt;a href=&quot;https://hexo.io/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Hexo&lt;/a&gt;! This is your very first post. Check &lt;a href=&quot;https://hexo.
      
    
    </summary>
    
    
  </entry>
  
</feed>
