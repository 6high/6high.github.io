<?xml version="1.0" encoding="utf-8"?>
<search>
    
    
    <entry>
        <title><![CDATA[The difference between function and method in scala]]></title>
        <url>http://6high.top/2018/04/01/The%20difference%20between%20function%20and%20method%20in%20scala/</url>
        <content type="html"><![CDATA[<h2 id="scala中function与method的区别"><a href="#scala中function与method的区别" class="headerlink" title="scala中function与method的区别"></a>scala中<code>function</code>与<code>method</code>的区别</h2><p>对于scala新手来说,一开始可能会含糊不清<strong>函数</strong>(<code>function</code>)和<strong>方法</strong>(<code>method</code>)的概念.在java8以前,我们说说的<strong>函数</strong>和<strong>方法</strong>是相同的概念,但是在scala中是不同的.</p>
<h2 id="首先我们来看个简单的例子"><a href="#首先我们来看个简单的例子" class="headerlink" title="首先我们来看个简单的例子:"></a>首先我们来看个简单的例子:</h2><h3 id="简单的-function-定义"><a href="#简单的-function-定义" class="headerlink" title="简单的 function 定义:"></a>简单的 <code>function</code> 定义:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> f1 = (x: <span class="type">Int</span>) =&gt; x + x</span><br><span class="line">f1: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1336</span>/<span class="number">975029298</span>@<span class="number">4</span>c18b432</span><br></pre></td></tr></table></figure>
<p>交互式scala印证了<code>Lambda</code>函数式编程,这是scala语言的精髓!函数可以作为参数被调用!极大的提高了编程效率.</p>
<h3 id="简单的method定义"><a href="#简单的method定义" class="headerlink" title="简单的method定义:"></a>简单的<code>method</code>定义:</h3><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">m1</span></span>(x: <span class="type">Int</span>):<span class="type">Int</span> = x + x</span><br><span class="line">m1: (x: <span class="type">Int</span>)<span class="type">Int</span></span><br></pre></td></tr></table></figure>
<p>可以看到<code>function</code>与<code>method</code>的定义并不同,<code>function</code>定义后,在内存中开辟了空间,我们可以将<code>f1</code>当做一个变量进行访问;而<code>m1</code>则跟我们传统接触到的函数相同,不能作为一个变量进行使用,<code>m1</code>方法只能在有参数的情况下才能使用.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; f1</span><br><span class="line">res9: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1336</span>/<span class="number">975029298</span>@<span class="number">4</span>c18b432</span><br><span class="line"></span><br><span class="line">scala&gt; m1</span><br><span class="line">&lt;console&gt;:<span class="number">13</span>: error: missing argument list <span class="keyword">for</span> method m1</span><br><span class="line"><span class="type">Unapplied</span> methods are only converted to functions when a function <span class="class"><span class="keyword">type</span> <span class="title">is</span> <span class="title">expected</span>.</span></span><br><span class="line"><span class="class"><span class="title">You</span> <span class="title">can</span> <span class="title">make</span> <span class="title">this</span> <span class="title">conversion</span> <span class="title">explicit</span> <span class="title">by</span> <span class="title">writing</span> `<span class="title">m1</span> <span class="title">_</span>` <span class="title">or</span> `<span class="title">m1</span>(<span class="params">_</span>)` <span class="title">instead</span> <span class="title">of</span> `<span class="title">m1</span>`.</span></span><br><span class="line"><span class="class">       <span class="title">m1</span></span></span><br><span class="line"><span class="class">       <span class="title">^</span></span></span><br></pre></td></tr></table></figure>
<h2 id="将method转换为function"><a href="#将method转换为function" class="headerlink" title="将method转换为function"></a>将<code>method</code>转换为<code>function</code></h2><p>根据直接运行<code>m1</code>后的错误提示我们可以知道,<code>method</code>只有被装换成<code>function</code>才能被允许.并且提示我们如何装换,仅仅在<code>m1</code>后面空格再加<code>_</code> :</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scala&gt; <span class="keyword">val</span> f2 = m1 _</span><br><span class="line">f2: <span class="type">Int</span> =&gt; <span class="type">Int</span> = $$<span class="type">Lambda</span>$<span class="number">1339</span>/<span class="number">61195378</span>@<span class="number">3</span>dbcdd4a</span><br></pre></td></tr></table></figure>
<p>我们来考虑下面的例子。 我们正在创建一个类型为<code>（Int，Int =&gt; Int）</code>的元组序列。 对于每个元组，我们决定将之前定义的方法转换为函数。 以下将创建10个实现相同功能的函数实例。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until <span class="number">10</span>) <span class="keyword">yield</span> i -&gt; m1 _</span><br></pre></td></tr></table></figure>
<p><strong>更好的方法是为每个元组传递相同的函数实例，并避免为以前的方法分配每个实例的内存</strong>。</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> (i &lt;- <span class="number">0</span> until <span class="number">10</span>) <span class="keyword">yield</span> i -&gt; f1</span><br></pre></td></tr></table></figure>
<h2 id="function与method的使用情景"><a href="#function与method的使用情景" class="headerlink" title="function与method的使用情景"></a><code>function</code>与<code>method</code>的使用情景</h2><ol>
<li>如果需要将它们<strong>作为参数</strong>传递，请使用<code>function</code>;</li>
<li>如果你想要对其实例进行操作，请使用<code>function</code>，例如:<code>f1.compose(f2)(2)</code>;</li>
<li>如果你想使用参数的默认值例如使用<code>method</code>，例如:<code>def m1(age: Int = 2)</code>;</li>
<li>如果你只需要计算并返回的功能，请使用<code>method</code>;</li>
<li>如果你需要抛出异常,请使用<code>method</code>,例如:</li>
</ol>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">@throws</span>(classOf[<span class="type">Exception</span>])</span><br><span class="line"><span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">play</span> </span>&#123;</span><br><span class="line">  <span class="comment">// exception throwing code here ...</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<hr>
<p><strong>参考文献:</strong><br>1.<a href="http://www.marcinkossakowski.com/difference-between-functions-and-methods-in-scala/#comment-7276" target="_blank" rel="noopener">Difference between functions and methods in Scala</a><br>2.<a href="https://alvinalexander.com/scala/scala-method-function-examples" target="_blank" rel="noopener">Scala method and function examples</a></p>
]]></content>
        
        <categories>
            
            <category> scala </category>
            
        </categories>
        
        
        <tags>
            
            <tag> scala </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[图解Spark wordCount]]></title>
        <url>http://6high.top/2018/03/10/%E5%9B%BE%E8%A7%A3Spark%20wordCount/</url>
        <content type="html"><![CDATA[<p>学习每一门新的语言都会先学习一个<code>helloworld</code>程序,而<a href="http://spark.apache.org/" target="_blank" rel="noopener">Spark官网</a>将world count作为一个入门程序,本屌也是一个spark菜鸟,那么就从一个word count开始吧!</p>
<p>==本文旨在用图解数据流程的方式帮助Spark新手理解RDD分区和基本算子==</p>
<h3 id="1-程序及数据文件"><a href="#1-程序及数据文件" class="headerlink" title="1.程序及数据文件"></a>1.程序及数据文件</h3><p>首先我们先填写一个包含单词的文件<code>helloSpark</code>,内容如下：</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hello <span class="type">Apache</span></span><br><span class="line">hello <span class="type">Spark</span></span><br><span class="line">hello <span class="type">Hadoop</span></span><br><span class="line"><span class="type">I</span> love <span class="type">Spark</span></span><br></pre></td></tr></table></figure>
<p>这个文件有四行,将作为我们word count程序的输入文件.<br>Word count程序答代码如下,代码中包含注释,方便大家理解.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> cn.hust.gao.wordCount</span><br><span class="line"><span class="keyword">import</span> org.apache.spark.&#123;<span class="type">SparkConf</span>, <span class="type">SparkContext</span>&#125;</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  	 <span class="comment">// 1.启动spark</span></span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="string">"WordCount"</span>)</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line">    <span class="comment">// 2.读取本地文件</span></span><br><span class="line">    <span class="keyword">val</span> input = <span class="string">"hdfs:///data/helloSpark"</span></span><br><span class="line">    <span class="comment">// 3.读取数据,将数据分为三个区域,并缓存</span></span><br><span class="line">    <span class="keyword">val</span> lines = sc.textFile(input, <span class="number">3</span>)</span><br><span class="line">    lines.cache()</span><br><span class="line">    <span class="comment">// 4.对每个分区的每一行数据进行切分,并压平</span></span><br><span class="line">    <span class="keyword">val</span> words = lines.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line">    <span class="comment">// 5.将单词映射成K-V形式</span></span><br><span class="line">    <span class="keyword">val</span> kv1 = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">    <span class="comment">// 6.对相同key的value进行合并</span></span><br><span class="line">    <span class="keyword">val</span> wordCount = kv1.reduceByKey(_ + _)</span><br><span class="line">    <span class="comment">// 7.对单词数量进行排序</span></span><br><span class="line">    <span class="keyword">val</span> result = wordCount.sortBy(_._2, <span class="literal">false</span>)</span><br><span class="line">    <span class="comment">// 8.收集数据打印</span></span><br><span class="line">    result.collect().foreach(println)</span><br><span class="line">    <span class="comment">// 让程序沉睡10000000毫秒,我们可以去 http://localhost:4040</span></span><br><span class="line">    <span class="comment">// 查看Spark UI界面</span></span><br><span class="line">    <span class="comment">// 关闭spark程序</span></span><br><span class="line">    <span class="type">Thread</span>.sleep(<span class="number">10000000</span>)</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><strong>注:</strong> 为了大家能方便的看到每个RDD各个分区的数据,推荐使用<code>mapPartitionsWithIndex</code>这个算子查看分区数据的方法:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># 打印dataStr这个<span class="type">RDD</span>总的数据</span><br><span class="line">dataStr.mapPartitionsWithIndex((index,iter) =&gt;</span><br><span class="line">  iter.toList.map(x =&gt; <span class="string">s"partID: <span class="subst">$index</span>, val: <span class="subst">$x</span>"</span>).iterator</span><br><span class="line">).collect().foreach(println)</span><br></pre></td></tr></table></figure>
<h3 id="2-源码分析和逻辑图解"><a href="#2-源码分析和逻辑图解" class="headerlink" title="2.源码分析和逻辑图解"></a>2.源码分析和逻辑图解</h3><h4 id="1-textFile"><a href="#1-textFile" class="headerlink" title="1. textFile"></a>1. textFile</h4><p>当我们通过<code>SparkContext</code>启动spark后,可通过<code>SparkContext.textFile()</code>读取数据源(可以是本地 file,内存数据结构, HDFS,HBase 等)的数据创建最初的<code>RDD</code>.本文从HDFS中读取数据,路径为<code>/data/helloSpark</code>,接下来我们阅读下<code>SparkContext.textFile()</code>的源码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">textFile</span></span>(</span><br><span class="line">    path: <span class="type">String</span>,</span><br><span class="line">    minPartitions: <span class="type">Int</span> = defaultMinPartitions): <span class="type">RDD</span>[<span class="type">String</span>] = withScope &#123;</span><br><span class="line">  assertNotStopped()</span><br><span class="line">  hadoopFile(path, classOf[<span class="type">TextInputFormat</span>], classOf[<span class="type">LongWritable</span>], classOf[<span class="type">Text</span>],</span><br><span class="line">    minPartitions).map(pair =&gt; pair._2.toString).setName(path)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>textFile()</code>需要数据路径<code>path</code>和分区数量<code>minPartitions</code>这两个参数,内部则是调用了<code>hadoopFile()</code>从HDFS中读取数据生成的<code>HadoopRDD[(LongWritable, Text)]</code>,经过<code>map</code>操作对每个分区的每行数据处理后获得<code>MapPartitionsRDD[String]</code><br>那么我们根据RDD lineage(RDD的血缘关系链)可以把<code>sc.textFile(input, 3)</code>的逻辑图画出来,如下:(<strong>矩形框代表RDD,矩形内部的圆和椭圆代表RDD的每个分区,圆形内是一行行的数据,这些分区分布不同的物理机器上</strong>)</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-1.textFile.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="1.textFile"></p>
<p><strong>注:</strong> 数据从HDFS中读取到内存中,会存在网络传输和交换数据,所以图中 HDFS -&gt; HadoopRDD 的箭头颜色为咖啡色;有网络交换数据的RDD会被标记成蓝色.</p>
<h4 id="2-flatMap-和-map"><a href="#2-flatMap-和-map" class="headerlink" title="2. flatMap 和 map"></a>2. flatMap 和 map</h4><p>通过<code>sc.textFile(input, 3)</code>把数据每一行读取成分为三个分区的<code>lines: RDD[String]</code>后,我们要将每一行数据通过<code>空格</code>进行切分,压平成每个单词,并对每个单词标记为<code>(word, 1)</code>这种K-V形式.下面我们阅读<code>flatMap</code>和<code>map</code>的源码.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">map</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">U</span>): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.map(cleanF))</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>从RDD的最简单的两个算子我们可以看出,RDD(Resilient Distributed Datasets),弹性分布式数据集其实就是跑在不同节点分区(不同物理机器)中的迭代器,通过网络通信将<code>Iterator</code>抽象成集合.<br><code>flatMap</code>和<code>map</code>的底层是RDD的每个分区(<code>MapPartitionsRDD</code>)中,调用scala迭代器的<code>Iterator.flatMap()</code>和<code>Iterator.map()</code>方法对该分区的数据进行处理.那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:<br><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-2.flatMap.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="2.flatMap"></p>
<h4 id="3-reduceByKey"><a href="#3-reduceByKey" class="headerlink" title="3. reduceByKey"></a>3. reduceByKey</h4><p>通过<code>lines.flatMap(line =&gt; line.split(&quot; &quot;))</code>对每行切分压平成单个单词后,使用<code>words.map(word =&gt; (word, 1))</code>将单词转换成K-V形式,接下来就是对相同的单词进行聚合,使用RDD的<code>reduceByKey</code>算子.我们先来看看源码,这次要带上注释了,因为这是wordCount最核心最核心的算子.</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   * Merge the values for each key using an associative and commutative reduce function. This will</span></span><br><span class="line"><span class="comment">   * also perform the merging locally on each mapper before sending results to a reducer, similarly</span></span><br><span class="line"><span class="comment">   * to a "combiner" in MapReduce. Output will be hash-partitioned with the existing partitioner/</span></span><br><span class="line"><span class="comment">   * parallelism level.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">    reduceByKey(defaultPartitioner(self), func)</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>
<p>通过阅读源码和注释,我们可以知道:<br>1.<code>reduceByKey</code>需要传入一个自定义聚合函数<code>func: (V, V) =&gt; V)</code>,这个聚合函数将会把两个相同 key 的 value 进行聚合. 如果我的自定义聚合函数是<code>func: (_ + _)</code>,即返回两个参数之和,那么<code>(hello, 1)</code>和<code>(hello, 1)</code>这两个key相同的数据,将会被聚合成<code>(hello, 2)</code>.</p>
<p>2.<code>reduceByKey</code>这个算子第一步将会在<strong>map端</strong>(即每个分区中)使用自定义聚合函数<code>func: (V, V) =&gt; V)</code>先聚合一次;</p>
<p>第二步将会对每个分区的key进行哈希运算,相同的key一定会被映射到相同的分区,不同的key可能被映射到相同的分区,然后将数据传输(shuffle)给指定的分区(reduce端)上去;</p>
<p>第三步,在reduce端,再调用制定与聚合函数<code>func: (V, V) =&gt; V)</code>即可完成RDD的聚合.</p>
<p>经过继续查看源码,我画出了如下逻辑图,大家先理解下.如下:</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-3.reduceByKey.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="3.reduceByKey"></p>
<p>我们可以看到<code>reduceByKey</code>还调用了<code>reduceByKey(defaultPartitioner(self), func)</code>,后面又调用了<code>combineByKeyWithClassTag[V]((v: V) =&gt; v, func, func, partitioner)</code>,那么我们再来看看源码:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">reduceByKey</span></span>(partitioner: <span class="type">Partitioner</span>, func: (<span class="type">V</span>, <span class="type">V</span>) =&gt; <span class="type">V</span>): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope &#123;</span><br><span class="line">  combineByKeyWithClassTag[<span class="type">V</span>]((v: <span class="type">V</span>) =&gt; v, func, func, partitioner)</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">combineByKeyWithClassTag</span></span>[<span class="type">C</span>](</span><br><span class="line">    createCombiner: <span class="type">V</span> =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeValue: (<span class="type">C</span>, <span class="type">V</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    mergeCombiners: (<span class="type">C</span>, <span class="type">C</span>) =&gt; <span class="type">C</span>,</span><br><span class="line">    partitioner: <span class="type">Partitioner</span>,</span><br><span class="line">    mapSideCombine: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    serializer: <span class="type">Serializer</span> = <span class="literal">null</span>)(<span class="keyword">implicit</span> ct: <span class="type">ClassTag</span>[<span class="type">C</span>]): <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">C</span>)] = self.withScope &#123;</span><br><span class="line">  require(mergeCombiners != <span class="literal">null</span>, <span class="string">"mergeCombiners must be defined"</span>) <span class="comment">// required as of Spark 0.9.0</span></span><br><span class="line">  <span class="keyword">if</span> (keyClass.isArray) &#123;</span><br><span class="line">    <span class="keyword">if</span> (mapSideCombine) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"Cannot use map-side combining with array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (partitioner.isInstanceOf[<span class="type">HashPartitioner</span>]) &#123;</span><br><span class="line">      <span class="keyword">throw</span> <span class="keyword">new</span> <span class="type">SparkException</span>(<span class="string">"HashPartitioner cannot partition array keys."</span>)</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">val</span> aggregator = <span class="keyword">new</span> <span class="type">Aggregator</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](</span><br><span class="line">    self.context.clean(createCombiner),</span><br><span class="line">    self.context.clean(mergeValue),</span><br><span class="line">    self.context.clean(mergeCombiners))</span><br><span class="line">  <span class="keyword">if</span> (self.partitioner == <span class="type">Some</span>(partitioner)) &#123;</span><br><span class="line">    self.mapPartitions(iter =&gt; &#123;</span><br><span class="line">      <span class="keyword">val</span> context = <span class="type">TaskContext</span>.get()</span><br><span class="line">      <span class="keyword">new</span> <span class="type">InterruptibleIterator</span>(context, aggregator.combineValuesByKey(iter, context))</span><br><span class="line">    &#125;, preservesPartitioning = <span class="literal">true</span>)</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">C</span>](self, partitioner)</span><br><span class="line">      .setSerializer(serializer)</span><br><span class="line">      .setAggregator(aggregator)</span><br><span class="line">      .setMapSideCombine(mapSideCombine)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p>我们可以看到<code>reduceByKey</code>最终调用的是<code>combineByKeyWithClassTag()</code>,<code>combineByKe</code>是很多RDD聚合算子的底层函数,学会<code>combineByKey</code>算子就学懂了spark基础的一半.</p>
<p>首先,<code>combineByKeyWithClassTag()</code>是强类型的算子,往里面传的自定义函数必须声明类型;其次这个算子有两个聚合过程,分别是<code>map端</code>和<code>reduce端</code>聚合,可以接受两个不同的聚合函数(<code>map端</code>和<code>reduce端</code>进行不同的聚合);第三必须传入一个将初始数据转换类型为聚合之后的类型的函数;第四,可以制定与分区器.</p>
<p>关于RDD中非常重要,Spark中非常重要的<code>combineByKeyWithClassTag()</code>算子,我之后也会出博客进行细讲!</p>
<h4 id="4-sortBy"><a href="#4-sortBy" class="headerlink" title="4. sortBy"></a>4. sortBy</h4><p>当我们算出了每个单词的数量后,剩下的任务就是排序了.Spark 的RDD 和javaRDD 提供了<code>sortByKey()</code>算子,但是我们的属性形式是<code>(word,counts)</code>,我们的单词才是key.如果是使用java,我们就只能在调用<code>sortByKey()</code>前将数据的K-V转调换为V-K.但是在scala中,提供了更加人性化的<code>sortBy()</code>算子,源码如下:</p>
<figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortBy</span></span>[<span class="type">K</span>](</span><br><span class="line">    f: (<span class="type">T</span>) =&gt; <span class="type">K</span>,</span><br><span class="line">    ascending: <span class="type">Boolean</span> = <span class="literal">true</span>,</span><br><span class="line">    numPartitions: <span class="type">Int</span> = <span class="keyword">this</span>.partitions.length)</span><br><span class="line">    (<span class="keyword">implicit</span> ord: <span class="type">Ordering</span>[<span class="type">K</span>], ctag: <span class="type">ClassTag</span>[<span class="type">K</span>]): <span class="type">RDD</span>[<span class="type">T</span>] = withScope &#123;</span><br><span class="line">  <span class="keyword">this</span>.keyBy[<span class="type">K</span>](f)</span><br><span class="line">      .sortByKey(ascending, numPartitions)</span><br><span class="line">      .values</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sortByKey</span></span>(ascending: <span class="type">Boolean</span> = <span class="literal">true</span>, numPartitions: <span class="type">Int</span> = self.partitions.length)</span><br><span class="line">    : <span class="type">RDD</span>[(<span class="type">K</span>, <span class="type">V</span>)] = self.withScope</span><br><span class="line">&#123;</span><br><span class="line">  <span class="keyword">val</span> part = <span class="keyword">new</span> <span class="type">RangePartitioner</span>(numPartitions, self, ascending)</span><br><span class="line">  <span class="keyword">new</span> <span class="type">ShuffledRDD</span>[<span class="type">K</span>, <span class="type">V</span>, <span class="type">V</span>](self, part)</span><br><span class="line">    .setKeyOrdering(<span class="keyword">if</span> (ascending) ordering <span class="keyword">else</span> ordering.reverse)</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>
<p><code>sortBy()</code>通过隐式转换(<code>implicit</code>)来控制排序,内部还是调用的<code>sortByKey()</code>, <code>sortByKey</code>会调用<code>RangePartitioner</code>分区器,将比较值的映射到不同的节点,数据计算交换传输(shuffle)到对应的节点,再在每个分区上进行排序.ascending=true表示升序，false表示降序。</p>
<p>那么我们根据RDD lineage 和数据流,可以画出逻辑图.如下:</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-4.sortBy.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="4.sortBy"></p>
<h3 id="3-全程逻辑图"><a href="#3-全程逻辑图" class="headerlink" title="3.全程逻辑图"></a>3.全程逻辑图</h3><p>高清的wordCount全程逻辑图,方便大家查看.</p>
<p><img src="http://p5igx3w0e.bkt.clouddn.com/2018-03-13-5.wordCount_logical_plan.png?imageView2/0/q/100|watermark/2/text/5b6u5L-h5Y-3OmxlYXJuaW5nU3Bhcms=/font/5a6L5L2T/fontsize/320/fill/IzBCMEMwQg==/dissolve/100/gravity/SouthEast/dx/10/dy/10" alt="5.wordCount_logical_plan"></p>
<p>最后感谢<a href="https://github.com/JerryLead" target="_blank" rel="noopener">@JerryLead</a>的Spark文档!</p>
]]></content>
        
        <categories>
            
            <category> spark </category>
            
            <category> RDD </category>
            
        </categories>
        
        
        <tags>
            
            <tag> RDD </tag>
            
            <tag> spark </tag>
            
        </tags>
        
    </entry>
    
    <entry>
        <title><![CDATA[Hello World]]></title>
        <url>http://6high.top/2017/05/16/hello-world/</url>
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>
<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
<h3 id="new-Document"><a href="#new-Document" class="headerlink" title="new Document"></a>new Document</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">title: 这是标题</span><br><span class="line">layout: post</span><br><span class="line">date: 2017-05-16 15:19:08</span><br><span class="line">comments: true</span><br><span class="line">tags: [test, spark]</span><br><span class="line">categories: [Paranoid, Web] </span><br><span class="line">keywords: [hexo, blog]</span><br><span class="line">---</span><br><span class="line"># this my first test</span><br></pre></td></tr></table></figure>
]]></content>
        
        
    </entry>
    
    
    
    
    
</search>
